{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Install Dependencies\n",
    "\n",
    "1. Installs the latest development version of Hugging Face's `diffusers` library directly from GitHub. This library is used for working with diffusion models such as Stable Diffusion.\n",
    "\n",
    "2. Installs and upgrades several core Hugging Face and optimization libraries:\n",
    "   - `transformers`: For using pre-trained language and vision models\n",
    "   - `accelerate`: Simplifies training across CPUs, GPUs, and distributed setups\n",
    "   - `wandb`: For logging experiments and visualizing training progress\n",
    "   - `bitsandbytes`: Enables 8-bit model loading for memory-efficient inference/training\n",
    "   - `peft`: For Parameter-Efficient Fine-Tuning of large models\n",
    "\n",
    "3. Installs additional Python libraries needed for data handling, model input/output, and networking or computer vision tasks:\n",
    "   - `pandas`: For data manipulation and tabular processing\n",
    "   - `torchvision`: For image transformations and loading datasets (used with PyTorch)\n",
    "   - `pyarrow`: For efficient I/O and working with Apache Arrow / Parquet formats\n",
    "   - `sentencepiece`: For subword tokenization used in many NLP models\n",
    "   - `controlnet_aux`: Adds support functions for ControlNet like HED, Canny, Depth, etc.\n",
    "   - `scapy`: For packet parsing and crafting, often used in networking/PCAP analysis\n",
    "   - `gdown`: For downloading files from Google Drive using file IDs\n",
    "   - `opencv-python`: For computer vision tasks and image manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \\\n",
    "    transformers \\\n",
    "    accelerate \\\n",
    "    wandb \\\n",
    "    bitsandbytes \\\n",
    "    peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas torchvision pyarrow sentencepiece controlnet_aux scapy gdown opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As SD3 is gated, before using it with diffusers you first need to go to the Stable Diffusion 3 Medium Hugging Face page, fill in the form and accept the gate. Once you are in, you need to log in so that your system knows you‚Äôve accepted the gate. Use the command below to log in:\n",
    "#Ignore if already logged in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication\n",
    "\n",
    "As SD3 is gated, you need to:\n",
    "1. Go to the [Stable Diffusion 3 Medium Hugging Face page](https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers)\n",
    "2. Fill in the form and accept the gate\n",
    "3. Log in to authenticate your access\n",
    "\n",
    "Check if you're already logged in first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not logged in, run this command and enter your token when prompted\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if shell commands work and show output\n",
    "!echo \"This should show output\"\n",
    "!pwd\n",
    "!ls -la | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use subprocess to capture output explicitly\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run_command(cmd):\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    print(f\"Command: {cmd}\")\n",
    "    print(f\"Return code: {result.returncode}\")\n",
    "    if result.stdout:\n",
    "        print(f\"Output: {result.stdout}\")\n",
    "    if result.stderr:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "\n",
    "# Test with a simple command\n",
    "run_command(\"echo 'Testing subprocess output'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force output to show in VS Code\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Test basic output\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# Test shell command with explicit flushing\n",
    "import subprocess\n",
    "result = subprocess.run(['echo', 'VS Code test'], capture_output=True, text=True)\n",
    "print(\"Shell output:\", result.stdout.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Example Dataset\n",
    "\n",
    "Alternatively, create your own dataset:\n",
    "1. Install nPrint in your environment (see https://nprint.github.io/nprint/)\n",
    "2. Collect PCAPs belonging to the same service/application you plan to model\n",
    "3. Convert each PCAP to an nPrint file with the following command:\n",
    "   ```\n",
    "   nprint -F -1 -P {pcap} -4 -i -6 -t -u -p 0 -c 1024 -W {output_file}\n",
    "   ```\n",
    "4. Place all resulting nPrint files in a folder named \"nprint_traffic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1vvneSH0a1WZFPHTafKusOUjNhg7oQioq --output preprocessed_dataset.zip\n",
    "!unzip -q preprocessed_dataset.zip\n",
    "!mkdir -p nprint_traffic\n",
    "!mv amazon_nprint_traffic/* nprint_traffic/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert nPrint to PNG Images\n",
    "\n",
    "This script transforms `.nprint` files‚Äîtabular feature representations of network packets‚Äîinto fixed-size PNG images that can be used as input for SD fine-tuning.\n",
    "\n",
    "**Input:** `.nprint` files generated from packet captures (PCAPs) using the nPrint tool. Each file is a CSV-like matrix where each row represents a single packet and columns correspond to extracted features.\n",
    "\n",
    "**Preprocessing:**\n",
    "- Drops IP address-related columns to avoid injecting identifiable or non-generalizable information into the model\n",
    "- Maps integer values in the remaining columns to RGBA color tuples to visualize numeric features as colored pixels\n",
    "\n",
    "**Padding:** Pads each image to a uniform height (default 1024) using a solid background to ensure model input consistency across varying packet counts.\n",
    "\n",
    "**Output:** Saves a PNG file for each `.nprint` file, preserving the packet structure as a vertically stacked color-coded image (rows = packets, cols = features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/nprint_to_png.py -i ./nprint_traffic/ -o ./nprint_traffic_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "Generates text prompt embeddings via a Stable Diffusion 3 pipeline and T5 text encoder. Maps each local PNG image to a unique SHA-256 hash and associates it with the computed embeddings. Stores the resulting image-hash-to-embedding data in a .parquet file for further processing.\n",
    "\n",
    "Here we are using the default instance prompt \"pixelated network data for type-0 application traffic\". You can configure this by referring to the compute_embeddings.py script for details on other supported arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/compute_embeddings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute embeddings:\n",
    "* Generates text prompt embeddings via a Stable Diffusion 3 pipeline and T5 text encoder.\n",
    "* Maps each local PNG image to a unique SHA-256 hash and associates it with the computed embeddings.\n",
    "* Stores the resulting image-hash-to-embedding data in a .parquet file for further processing.\n",
    "* Here we are using the default instance prompt \"pixelated network data for type-0 application traffic\".\n",
    "* But you can configure this. Refer to the compute_embeddings.py script for details on other supported arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/compute_embeddings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LoRA Adapter on Stable Diffusion 3 (Miniature Setup)\n",
    "\n",
    "This command launches training using `accelerate` with DreamBooth-style LoRA tuning, optimized for quick experimentation or demo runs.\n",
    "\n",
    "**‚ö†Ô∏è Current configuration uses:**\n",
    "- Only 1 training step\n",
    "- Small batch size\n",
    "- No warmup\n",
    "- High learning rate\n",
    "\n",
    "**Intended only for testing or verifying training scripts**, NOT for quality results. For actual training, increase `max_train_steps`, adjust learning rate, and consider enabling full validation and saving checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch ./scripts/train_dreambooth_lora_sd3_miniature.py \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-3-medium-diffusers\"  \\\n",
    "  --instance_data_dir=\"nprint_traffic_images\" \\\n",
    "  --data_df_path=\"sample_embeddings.parquet\" \\\n",
    "  --output_dir=\"trained-sd3-lora-miniature\" \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --instance_prompt=\"pixelated network data for type-0 application traffic\" \\\n",
    "  --train_batch_size=2 \\\n",
    "  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
    "  --use_8bit_adam \\\n",
    "  --learning_rate=5e-5 \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=1 \\\n",
    "  --seed=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline for SD3 + ControlNet + LoRA\n",
    "\n",
    "This cell demonstrates the full generation process:\n",
    "- Loads Stable Diffusion 3 Medium base model\n",
    "- Loads ControlNet (Canny edge-based guidance)\n",
    "- Loads LoRA weights fine-tuned on pixelated network traffic\n",
    "- Applies edge-conditioned generation using a sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush()\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusion3ControlNetPipeline, SD3ControlNetModel\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# Make sure our output folder exists\n",
    "os.makedirs(\"generated_traffic_images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base SD 3.0 model\n",
    "base_model_path = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "\n",
    "# Canny-based ControlNet\n",
    "controlnet_path = \"InstantX/SD3-Controlnet-Canny\"\n",
    "\n",
    "# Load the ControlNet and pipeline\n",
    "controlnet = SD3ControlNetModel.from_pretrained(\n",
    "    controlnet_path, torch_dtype=torch.float16\n",
    ")\n",
    "pipe = StableDiffusion3ControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "lora_output_path = \"trained-sd3-lora-miniature\"\n",
    "pipe.load_lora_weights(lora_output_path)\n",
    "\n",
    "# Move pipeline to GPU (half precision)\n",
    "pipe.to(\"cuda\", torch.float16)\n",
    "pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original control image to Canny edges via OpenCV\n",
    "orig_path = \"./scripts/traffic_conditioning_image.png\"\n",
    "orig_bgr = cv2.imread(orig_path)\n",
    "if orig_bgr is None:\n",
    "    raise ValueError(f\"Could not load file: {orig_path}\")\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(orig_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Generate Canny edge map (tweak thresholds as needed)\n",
    "edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "# Convert single-channel edge map to 3-channel RGB\n",
    "edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Convert to PIL for use in ControlNet pipeline\n",
    "control_image = Image.fromarray(edges_rgb)\n",
    "orig_width, orig_height = control_image.size\n",
    "target_width = 1024\n",
    "if orig_width > target_width:\n",
    "    # (left, upper, right, lower)\n",
    "    control_image = control_image.crop((0, 0, target_width, orig_height))\n",
    "    \n",
    "print(\"Displaying Canny control image:\")\n",
    "display(control_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Generation Processing Pipeline\n",
    "\n",
    "This cell performs a 3-stage transformation of the generated PNG images:\n",
    "\n",
    "1. **Color Augmentation:** Applies standardized color shifts to improve nprint reconstruction accuracy\n",
    "2. **Image-to-nPrint Conversion:** Converts augmented images back into nPrint-compatible feature format using a reference file to maintain consistent structure\n",
    "3. **Heuristic Correction & PCAP Reconstruction:** Reconstructs valid and replayable PCAP files from the diffusion-generated nPrint representation\n",
    "\n",
    "This pipeline enables turning synthetic traffic images back into replayable network traffic for evaluation or simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Color Augmentation\n",
    "!python ./scripts/color_processor.py \\\n",
    "  --input_dir=\"./generated_traffic_images\" \\\n",
    "  --output_dir=\"./color_corrected_generated_traffic_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Image-to-nPrint Conversion\n",
    "!python ./scripts/image_to_nprint.py \\\n",
    "  --org_nprint ./scripts/column_example.nprint \\\n",
    "  --input_dir ./color_corrected_generated_traffic_images \\\n",
    "  --output_dir ./generated_nprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Heuristic Correction & PCAP Reconstruction\n",
    "!python ./scripts/mass_reconstruction.py \\\n",
    "  --input_dir ./generated_nprint \\\n",
    "  --output_pcap_dir ./replayable_generated_pcaps \\\n",
    "  --output_nprint_dir ./replayable_generated_nprints \\\n",
    "  --formatted_nprint_path ./scripts/correct_format.nprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final PCAP files are stored in `replayable_generated_pcaps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# üéØ Inference Pipeline for SD3 + ControlNet + LoRA\n",
    "#\n",
    "# This cell demonstrates the full generation process:\n",
    "#  - Loads Stable Diffusion 3 Medium base model\n",
    "#  - Loads ControlNet (Canny edge-based guidance)\n",
    "#  - Loads LoRA weights fine-tuned on pixelated network traffic\n",
    "#  - Applies edge-conditioned generation using a sample input\n",
    "#\n",
    "# -----------------------------------------------------------\n",
    "flush()\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusion3ControlNetPipeline, SD3ControlNetModel\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# Make sure our output folder exists\n",
    "os.makedirs(\"generated_traffic_images\", exist_ok=True)\n",
    "\n",
    "# Base SD 3.0 model\n",
    "base_model_path = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "\n",
    "# Canny-based ControlNet\n",
    "controlnet_path = \"InstantX/SD3-Controlnet-Canny\"\n",
    "\n",
    "# Load the ControlNet and pipeline\n",
    "controlnet = SD3ControlNetModel.from_pretrained(\n",
    "    controlnet_path, torch_dtype=torch.float16\n",
    ")\n",
    "pipe = StableDiffusion3ControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "lora_output_path = \"trained-sd3-lora-miniature\"\n",
    "pipe.load_lora_weights(lora_output_path)\n",
    "\n",
    "# Move pipeline to GPU (half precision)\n",
    "pipe.to(\"cuda\", torch.float16)\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) Convert original control image to Canny edges via OpenCV\n",
    "# ----------------------------------------------------\n",
    "orig_path = \"./scripts/traffic_conditioning_image.png\"\n",
    "orig_bgr = cv2.imread(orig_path)\n",
    "if orig_bgr is None:\n",
    "    raise ValueError(f\"Could not load file: {orig_path}\")\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(orig_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Generate Canny edge map (tweak thresholds as needed)\n",
    "edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "# Convert single-channel edge map to 3-channel RGB\n",
    "edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Convert to PIL for use in ControlNet pipeline\n",
    "control_image = Image.fromarray(edges_rgb)\n",
    "orig_width, orig_height = control_image.size\n",
    "target_width = 1024\n",
    "if orig_width > target_width:\n",
    "    # (left, upper, right, lower)\n",
    "    control_image = control_image.crop((0, 0, target_width, orig_height))\n",
    "    \n",
    "print(\"Displaying Canny control image:\")\n",
    "\n",
    "display(control_image)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Set up prompts and run pipeline\n",
    "# ----------------------------------------------------\n",
    "prompt = \"pixelated network data for type-0 application traffic\"\n",
    "generator = torch.manual_seed(0)  # reproducibility\n",
    "\n",
    "# Generate at 1024√ó1024 to match the new control image\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=20,\n",
    "    generator=generator,\n",
    "    height=1024,\n",
    "    width=1088,\n",
    "    control_image=control_image,\n",
    "    controlnet_conditioning_scale=0.5,  # increase to adhere more strongly to edges\n",
    ").images[0]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) Save the generated image\n",
    "# ----------------------------------------------------\n",
    "output_path = os.path.join(\"generated_traffic_images\", \"generated_traffic.png\")\n",
    "image.save(output_path)\n",
    "print(f\"Generated image saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# üîÑ Post-Generation Processing Pipeline\n",
    "#\n",
    "# This cell performs a 3-stage transformation of the generated PNG images:\n",
    "#   1. Applies color correction for standardization.\n",
    "#   2. Converts augmented images back into nPrint-compatible feature format.\n",
    "#   3. Applies heuristic corrections and reconstructs valid PCAP files.\n",
    "#\n",
    "# ‚öôÔ∏è This pipeline enables turning synthetic traffic images\n",
    "#    back into replayable network traffic for evaluation or simulation.\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# üé® Step 1: Color Augmentation\n",
    "# Applies standardized color shifts to improve nprint reconstruction accuracy\n",
    "!python ./scripts/color_processor.py \\\n",
    "  --input_dir=\"./generated_traffic_images\" \\\n",
    "  --output_dir=\"./color_corrected_generated_traffic_images\"\n",
    "# -----------------------------------------------------------\n",
    "# üîÅ Step 2: Image-to-nPrint Conversion\n",
    "# Converts augmented PNG images back into `.nprint` tabular format.\n",
    "#\n",
    "# Uses a reference `.nprint` file to maintain consistent structure and column order.\n",
    "# This step allows diffusion-generated visual traffic to be fed into analysis tools.\n",
    "# -----------------------------------------------------------\n",
    "!python ./scripts/image_to_nprint.py \\\n",
    "  --org_nprint ./scripts/column_example.nprint \\\n",
    "  --input_dir ./color_corrected_generated_traffic_images \\\n",
    "  --output_dir ./generated_nprint\n",
    "# -----------------------------------------------------------\n",
    "# üß† Step 3: Heuristic Correction & PCAP Reconstruction\n",
    "#\n",
    "# This step reconstructs a valid and replayable `.pcap` file\n",
    "# from the diffusion-generated `.nprint` representation.\n",
    "# üîç Core Functionalities:\n",
    "# ‚úÖ Intra-packet corrections (fixes within individual packets).\n",
    "# üîÅ Inter-packet dependency enforcement.\n",
    "# üîß Reconstruction:\n",
    "#   - Save the corrected `.nprint` to disk\n",
    "#   - Call `nprint -W` to convert `.nprint` into `.pcap` using external tool\n",
    "#   - Run Scapy-based checksum updates to ensure IPv4 validity\n",
    "#   - Reconvert final `.pcap` back to `.nprint` (with fixed layout) for downstream tasks\n",
    "# -----------------------------------------------------------\n",
    "!python ./scripts/mass_reconstruction.py \\\n",
    "  --input_dir ./generated_nprint \\\n",
    "  --output_pcap_dir ./replayable_generated_pcaps \\\n",
    "  --output_nprint_dir ./replayable_generated_nprints \\\n",
    "  --formatted_nprint_path ./scripts/correct_format.nprint\n",
    "# Final Pcap is stored in replayable_generated_pcaps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
